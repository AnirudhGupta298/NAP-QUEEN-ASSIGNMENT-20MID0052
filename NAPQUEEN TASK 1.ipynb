{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad663c4-dfae-40bd-bc10-6d8bd465c37f",
   "metadata": {},
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a0ef4-804b-4718-8eeb-8f664b8aeaaa",
   "metadata": {},
   "source": [
    "### Given a dummy sales data of a well-known brand on Amazon ,building a time series forecasting model that predicts the number of units sold for each item ID.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701707b7-9b0d-455c-bb15-fef160c85066",
   "metadata": {},
   "source": [
    "### EXPLORATORY DATA ANALYSIS (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b7897c5-52ef-49fb-9785-ee49bb51e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Loading the dataset\n",
    "data = pd.read_csv('train.csv')\n",
    "# Converting 'date' column to datetime format\n",
    "data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "# Extracting date-related features\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "data['dayofweek'] = data['date'].dt.dayofweek\n",
    "# Sorting the data by date and item ID\n",
    "data = data.sort_values(by=['Item Id', 'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16188732-8c85-4b90-89af-28a008ac75b0",
   "metadata": {},
   "source": [
    "### FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dc411db-fc8b-4239-b79a-9b4fe5f0c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping 'Item Id' from the features\n",
    "features = ['year', 'month', 'day', 'dayofweek', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'rolling_mean_7', 'rolling_sum_7', 'ad_spend']\n",
    "target = 'units'\n",
    "# Creating lag features for the data\n",
    "for lag in range(1, 8):\n",
    "    data[f'lag_{lag}'] = data.groupby('Item Id')['units'].shift(lag)\n",
    "# Creating rolling/window features for the data\n",
    "data['rolling_mean_7'] = data.groupby('Item Id')['units'].transform(lambda x: x.rolling(window=7).mean())\n",
    "data['rolling_sum_7'] = data.groupby('Item Id')['units'].transform(lambda x: x.rolling(window=7).sum())\n",
    "# Dropping rows with NaN values generated by lag and rolling features\n",
    "data.dropna(inplace=True)\n",
    "# Selecting features and target variable\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "# Loading the test dataset\n",
    "test_data = pd.read_csv('test.csv')\n",
    "# Converting 'date' column to datetime format\n",
    "test_data['date'] = pd.to_datetime(test_data['date'], format='%Y-%m-%d')\n",
    "# Extracting date-related features\n",
    "test_data['year'] = test_data['date'].dt.year\n",
    "test_data['month'] = test_data['date'].dt.month\n",
    "test_data['day'] = test_data['date'].dt.day\n",
    "test_data['dayofweek'] = test_data['date'].dt.dayofweek\n",
    "# Ensuring all necessary columns exist in the test set\n",
    "for lag in range(1, 8):\n",
    "    test_data[f'lag_{lag}'] = np.nan\n",
    "test_data['rolling_mean_7'] = np.nan\n",
    "test_data['rolling_sum_7'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f464dd-5c12-424c-a5b8-1cdbe427c674",
   "metadata": {},
   "source": [
    "### MODEL SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e9ec6-b008-48bc-96f8-5d79b3b10fa6",
   "metadata": {},
   "source": [
    "XGBoost is a gradient boosting model known for its high performance in regression tasks. It excels with its ability to handle large datasets and capture intricate relationships between features through boosting techniques. The model's flexibility in handling various types of data and its ability to fine-tune hyperparameters make it a popular choice for predictive analytics.\n",
    "\n",
    "LightGBM is another gradient boosting framework optimized for speed and efficiency, particularly with large datasets. It uses histogram-based algorithms to enhance training speed and reduce memory usage, making it suitable for time series forecasting where data size can be substantial\n",
    "\n",
    "Voting Regressor is employed to combine the strengths of both models. By averaging predictions from XGBoost and LightGBM, the Voting Regressor can potentially improve overall accuracy and stability, leveraging the individual model strengths and mitigating their weaknesses. This ensemble approach often results in better generalization and more reliable predictions compared to single models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "626119e5-21cd-46b0-93fe-f71019115981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Mean Squared Error: 5841.064198659013\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2606\n",
      "[LightGBM] [Info] Number of data points in the train set: 41694, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 13.272677\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2606\n",
      "[LightGBM] [Info] Number of data points in the train set: 41694, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 13.272677\n",
      "Voting Regressor Mean Squared Error: 4971.752984199898\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "# Splitting the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Training an XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "# Making predictions and evaluate the model\n",
    "y_pred = xgb_model.predict(X_val)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print(f'XGBoost Mean Squared Error: {mse}')\n",
    "# Training a LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor()\n",
    "lgb_model.fit(X_train, y_train)\n",
    "# preparing an Ensemble model with Voting Regressor\n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('xgb', xgb_model),\n",
    "    ('lgb', lgb_model)\n",
    "])\n",
    "voting_model.fit(X_train, y_train)\n",
    "y_pred_voting = voting_model.predict(X_val)\n",
    "mse_voting = mean_squared_error(y_val, y_pred_voting)\n",
    "print(f'Voting Regressor Mean Squared Error: {mse_voting}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b713abb-64d4-4a51-9cb7-e09fd62ee244",
   "metadata": {},
   "source": [
    "### HYPER PARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39bc52a8-830c-42cb-b0dd-3313d221a86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Regressor Mean Squared Error: 4971.752984199898\n",
      "Final XGBoost Mean Squared Error: 6895.246074568306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred_voting = voting_model.predict(X_val)\n",
    "mse_voting = mean_squared_error(y_val, y_pred_voting)\n",
    "print(f'Voting Regressor Mean Squared Error: {mse_voting}')\n",
    "\n",
    "# Hyperparameter tuning for XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "final_xgb_model = xgb.XGBRegressor(objective='reg:squarederror', **best_params)\n",
    "final_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions and evaluating the final XGBoost model\n",
    "y_pred_final = final_xgb_model.predict(X_val)\n",
    "mse_final = mean_squared_error(y_val, y_pred_final)\n",
    "print(f'Final XGBoost Mean Squared Error: {mse_final}')\n",
    "\n",
    "# Making predictions on the test dataset\n",
    "X_test = test_data[features]\n",
    "test_data['TARGET'] = voting_model.predict(X_test)\n",
    "\n",
    "# Creating the submission file\n",
    "submission = test_data[['date', 'Item Id', 'TARGET']]\n",
    "submission.to_csv('Task1_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ffcf55-f480-4a9a-8977-5a12e9870557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
